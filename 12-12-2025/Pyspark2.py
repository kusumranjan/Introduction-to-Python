# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xz-dRHpIU337x__8m_BYwbm7UVf7xxTP
"""

!pip install pyspark

from pyspark.sql import SparkSession

spark=SparkSession.builder\
.appName("Basics")\
.getOrCreate()

data = [
("C001", "Arjun", "Hyderabad", 25, 45000, "Electronics"),
("C002", "Meera", "Chennai", 32, 52000, "Grocery"),
("C003", "Rajesh", "Bangalore", 29, 61000, "Clothing"),
("C004", "Priya", "Delhi", 22, 38000, "Grocery"),
("C005", "Sanjay", "Mumbai", 35, 72000, "Electronics"),
("C006", "Kavya", "Hyderabad", 28, 48000, "Grocery"),
("C007", "Imran", "Delhi", 31, 53000, "Clothing"),
("C008", "Divya", "Chennai", 27, 45000, "Electronics"),
("C009", "Anil", "Bangalore", 40, 85000, "Furniture"),
("C010", "Ritu", "Mumbai", 23, 39000, "Clothing"),
("C011", "Hari", "Hyderabad", 33, 56000, "Grocery"),
("C012", "Sana", "Delhi", 26, 47000, "Electronics"),
("C013", "Vikram", "Chennai", 38, 91000, "Furniture"),
("C014", "Deepa", "Mumbai", 30, 62000, "Clothing"),
("C015", "Asha", "Bangalore", 24, 41000, "Grocery"),
("C016", "Kiran", "Delhi", 29, 59000, "Furniture"),
("C017", "Farah", "Hyderabad", 36, 70000, "Clothing"),
("C018", "Tarun", "Chennai", 28, 53000, "Furniture"),
("C019", "Nisha", "Mumbai", 21, 35000, "Grocery"),
("C020", "Yusuf", "Bangalore", 34, 76000, "Electronics"),
("C021", "Pooja", "Delhi", 27, 47000, "Clothing"),
("C022", "Zara", "Hyderabad", 32, 58000, "Grocery"),
("C023", "Ajay", "Chennai", 30, 51000, "Furniture"),
("C024", "Reema", "Bangalore", 28, 49000, "Clothing"),
("C025", "Gautam", "Mumbai", 39, 82000, "Furniture"),
("C026", "Swati", "Delhi", 25, 46000, "Electronics"),
("C027", "Mahesh", "Hyderabad", 41, 90000, "Furniture"),
("C028", "Anita", "Chennai", 26, 44000, "Clothing"),
("C029", "Sameer", "Bangalore", 33, 68000, "Electronics"),
("C030", "Leela", "Delhi", 22, 36000, "Grocery")
]
columns = ["customer_id", "name", "city", "age", "annual_spend", "category"]

df = spark.createDataFrame(data, columns)
df.show()
-------------------------------------------------
df.show(10)
-------------------------------------------------
df.select("city").distinct().show()
-----------------------------------------------
df.select("customer_id", "name","annual_spend").show()
---------------------------------------------------
df.filter(df.annual_spend > 60000).show()
-------------------------------------------------
df.filter(df.age<30).select("name").show()
------------------------------------------------------
from pyspark.sql.functions import col
df2 = df.withColumn("spend_lakh", col("annual_spend") / 100000)
df2.show()
------------------------------------------------------------
from pyspark.sql.functions import col, when
df2 = df.withColumn(
    "customer_type",
       when(col("annual_spend") > 70000, "Premium").otherwise("Standard")
)
df2.show()
-----------------------------------------------------
df.filter(df.name.startswith("A")).show()
-------------------------------------------------------
df.filter((df.category == "Clothing") | (df.category == "Electronics")).show()
-----------------------------------------------------
from pyspark.sql.functions import upper
df3=df.withColumn("city_upper",upper(col("city")))
df3.show()
--------------------------------------
df=df.drop("Category")
df.show()
------------------------------------------
df.orderBy(df.age.desc()).show()
----------------------------------------
from pyspark.sql.functions import avg,col

df2=df.select(avg("age")).collect()[0][0]
df.filter(col("age") < df2).show()
------------------------------------------
df.orderBy(df.annual_spend.desc()).show(5)
-----------------------------------------------
from pyspark.sql.functions import upper

df_mumbai = df.filter(df.city == "Mumbai")
df_mumbai.show()
------------------------------------------------
city_counts = df.groupBy("city").count().show()
---------------------------------------------------
from pyspark.sql.functions import col
df.withColumn("is_senior",col("age")>35).show()
---------------------------------------------------
from pyspark.sql import functions as F

df.withColumn("first_letter", F.substring(F.col("name"), 1, 1)).show()
