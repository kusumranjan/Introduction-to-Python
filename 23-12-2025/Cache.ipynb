{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Cache"
      ],
      "metadata": {
        "id": "ljI1J3ORcia7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lh6q8QPscNY8"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Cache\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"ORD001\",\"Delhi\",\"Laptop\",45000),\n",
        "    (\"ORD002\",\"Mumbai\",\"Mobile\",32000),\n",
        "    (\"ORD003\",\"Bangalore\",\"Laptop\",52000),\n",
        "    (\"ORD004\",\"Delhi\",\"Tablet\",28000),\n",
        "    (\"ORD005\",\"Mumbai\",\"Laptop\",61000),\n",
        "    (\"ORD006\",\"Chennai\",\"Mobile\",30000),\n",
        "    (\"ORD007\",\"Delhi\",\"Laptop\",47000),\n",
        "    (\"ORD008\",\"Bangalore\",\"Tablet\",35000),\n",
        "    (\"ORD009\",\"Mumbai\",\"Laptop\",58000),\n",
        "    (\"ORD010\",\"Delhi\",\"Mobile\",29000)\n",
        "]\n",
        "\n",
        "columns = [\"order_id\",\"city\",\"product\",\"amount\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)"
      ],
      "metadata": {
        "id": "d8Bq7_W_ckaq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_value_df = df.filter(df.amount > 30000)"
      ],
      "metadata": {
        "id": "3meqCwxfcyGn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_value_df.count()\n",
        "high_value_df.groupBy(\"city\").sum(\"amount\").show()\n",
        "high_value_df.groupBy(\"product\").avg(\"amount\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4GRRvUBc8zV",
        "outputId": "314e7693-4c50-4b6c-f75f-d387dbff11d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+\n",
            "|     city|sum(amount)|\n",
            "+---------+-----------+\n",
            "|Bangalore|      87000|\n",
            "|   Mumbai|     151000|\n",
            "|    Delhi|      92000|\n",
            "+---------+-----------+\n",
            "\n",
            "+-------+-----------+\n",
            "|product|avg(amount)|\n",
            "+-------+-----------+\n",
            "| Laptop|    52600.0|\n",
            "| Mobile|    32000.0|\n",
            "| Tablet|    35000.0|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "high_value_df.cache()\n",
        "\n",
        "high_value_df.count()\n",
        "high_value_df.groupBy(\"city\").sum(\"amount\").show()\n",
        "high_value_df.groupBy(\"product\").avg(\"amount\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMk7rm2Jdc6B",
        "outputId": "d2237180-bdbc-40ed-8226-6ea063bb25e6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+\n",
            "|     city|sum(amount)|\n",
            "+---------+-----------+\n",
            "|Bangalore|      87000|\n",
            "|   Mumbai|     151000|\n",
            "|    Delhi|      92000|\n",
            "+---------+-----------+\n",
            "\n",
            "+-------+-----------+\n",
            "|product|avg(amount)|\n",
            "+-------+-----------+\n",
            "| Laptop|    52600.0|\n",
            "| Mobile|    32000.0|\n",
            "| Tablet|    35000.0|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "high_value_df.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErUCpyt9d0-G",
        "outputId": "c2f31755-1f0b-49d8-9b34-cee3e3249164"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[order_id: string, city: string, product: string, amount: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.filter(df.amount > 30000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "B23ktdo3hBb3",
        "outputId": "c38bc463-e75b-4d66-e693-55d9c4f2fc96"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'filter'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3680703057.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamount\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m30000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'filter'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "BFJ0s99thvnZ",
        "outputId": "b7f1191c-3ebf-4a31-9c64-92e54a6350aa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'count'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2255720674.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'count'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "exercise - 1"
      ],
      "metadata": {
        "id": "-C-FxrNngp1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PySpark setup (once)\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType, IntegerType, DateType\n",
        ")\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RetailChainCapstone\").getOrCreate()\n",
        "\n",
        "# Provided raw data (simulating CSV/JSON ingestion)\n",
        "sales_data = [\n",
        "    (\"TXN001\",\"Delhi \",\"Laptop\",\"Electronics\",\"45000\",\"2024-01-05\",\"Completed\"),\n",
        "    (\"TXN002\",\"Mumbai\",\"Mobile \",\"electronics\",\"32000\",\"05/01/2024\",\"Completed\"),\n",
        "    (\"TXN003\",\"Bangalore\",\"Tablet\",\" Electronics \",\"30000\",\"2024/01/06\",\"Completed\"),\n",
        "    (\"TXN004\",\"Delhi\",\"Laptop\",\"Electronics\",\"\",\"2024-01-07\",\"Cancelled\"),\n",
        "    (\"TXN005\",\"Chennai\",\"Mobile\",\"Electronics\",\"invalid\",\"2024-01-08\",\"Completed\"),\n",
        "    (\"TXN006\",\"Mumbai\",\"Tablet\",\"Electronics\",None,\"2024-01-08\",\"Completed\"),\n",
        "    (\"TXN007\",\"Delhi\",\"Laptop\",\"electronics\",\"45000\",\"09-01-2024\",\"Completed\"),\n",
        "    (\"TXN008\",\"Bangalore\",\"Mobile\",\"Electronics\",\"28000\",\"2024-01-09\",\"Completed\"),\n",
        "    (\"TXN009\",\"Mumbai\",\"Laptop\",\"Electronics\",\"55000\",\"2024-01-10\",\"Completed\"),\n",
        "    (\"TXN009\",\"Mumbai\",\"Laptop\",\"Electronics\",\"55000\",\"2024-01-10\",\"Completed\")\n",
        "]\n",
        "\n",
        "customer_data = [\n",
        "    (\"C001\",\"Delhi\",\"Premium\"),\n",
        "    (\"C002\",\"Mumbai\",\"Standard\"),\n",
        "    (\"C003\",\"Bangalore\",\"Premium\"),\n",
        "    (\"C004\",\"Chennai\",\"Standard\"),\n",
        "    (\"C005\",\"Mumbai\",\"Premium\")\n",
        "]\n",
        "\n",
        "city_lookup = [\n",
        "    (\"Delhi\",\"Tier-1\"),\n",
        "    (\"Mumbai\",\"Tier-1\"),\n",
        "    (\"Bangalore\",\"Tier-1\"),\n",
        "    (\"Chennai\",\"Tier-2\")\n",
        "]\n"
      ],
      "metadata": {
        "id": "u0NwcDJlCtLa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 1 — DATA INGESTION & SCHEMA MANAGEMENT"
      ],
      "metadata": {
        "id": "pienY0R8g9ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sales_schema = StructType([\n",
        "    StructField(\"txn_id\",    StringType(), True),\n",
        "    StructField(\"city\",      StringType(), True),\n",
        "    StructField(\"product\",   StringType(), True),\n",
        "    StructField(\"category\",  StringType(), True),\n",
        "    # ingest amount as string to gracefully handle invalid values\n",
        "    StructField(\"amount\",    StringType(), True),\n",
        "    # ingest txn_date as string (multiple formats will be parsed later)\n",
        "    StructField(\"txn_date\",  StringType(), True),\n",
        "    StructField(\"status\",    StringType(), True),\n",
        "])\n",
        "\n",
        "customer_schema = StructType([\n",
        "    StructField(\"customer_id\", StringType(), True),\n",
        "    StructField(\"city\",        StringType(), True),\n",
        "    StructField(\"segment\",     StringType(), True),\n",
        "])\n",
        "\n",
        "city_schema = StructType([\n",
        "    StructField(\"city\", StringType(), True),\n",
        "    StructField(\"tier\", StringType(), True),\n",
        "])\n"
      ],
      "metadata": {
        "id": "ZQYTvwtDg3JI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load raw data into DataFrames"
      ],
      "metadata": {
        "id": "OML1uBcuhMFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sales_df_raw = spark.createDataFrame(sales_data, schema=sales_schema)\n",
        "customer_df_raw = spark.createDataFrame(customer_data, schema=customer_schema)\n",
        "city_df = spark.createDataFrame(city_lookup, schema=city_schema)\n",
        "\n",
        "# If reading from files instead:\n",
        "# sales_df_raw = spark.read.schema(sales_schema).option(\"mode\", \"PERMISSIVE\").csv(\"/path/sales.csv\")\n",
        "# customer_df_raw = spark.read.schema(customer_schema).json(\"/path/customer.json\")\n",
        "# city_df = spark.read.schema(city_schema).csv(\"/path/city_lookup.csv\")\n"
      ],
      "metadata": {
        "id": "mjp6_ZYdhT_8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Handle incorrect data types gracefully"
      ],
      "metadata": {
        "id": "Imt0K_AemuFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example safe amount parse (digits only)\n",
        "sales_df_typed = sales_df_raw.withColumn(\n",
        "    \"amount_int\",\n",
        "    F.when(F.col(\"amount\").isNull(), None)\n",
        "     .otherwise(\n",
        "        F.when(F.length(F.regexp_replace(F.col(\"amount\"), r\"[^0-9]\", \"\")) > 0,\n",
        "               F.regexp_replace(F.col(\"amount\"), r\"[^0-9]\", \"\").cast(IntegerType()))\n",
        "         .otherwise(None)\n",
        "     )\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZVw9N4ekmvYV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identify corrupt and invalid records"
      ],
      "metadata": {
        "id": "iPVKCscLm2km"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 2 — DATA CLEANING & TRANSFORMATION\n",
        "5. Trim and normalize string columns"
      ],
      "metadata": {
        "id": "p3qZL-kmnDt4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "xyRGHNv5m7fM",
        "outputId": "6ad861cf-89d8-4d4f-9824-7e3f50eabf3a"
      },
      "source": [
        "parsed_timestamp = F.coalesce(\n",
        "    F.try_to_timestamp(F.col(\"txn_date\"), \"yyyy-MM-dd\"),\n",
        "    F.try_to_timestamp(F.col(\"txn_date\"), \"dd/MM/yyyy\"),\n",
        "    F.try_to_timestamp(F.col(\"txn_date\"), \"yyyy/MM/dd\"),\n",
        "    F.try_to_timestamp(F.col(\"txn_date\"), \"dd-MM-yyyy\")\n",
        ")\n",
        "parsed_date = parsed_timestamp.cast(DateType())\n",
        "\n",
        "sales_df_flagged = sales_df_typed.withColumn(\"event_date\", parsed_date).withColumn(\n",
        "    \"is_invalid_amount\", F.col(\"amount_int\").isNull()\n",
        ").withColumn(\n",
        "    \"is_invalid_date\", F.col(\"event_date\").isNull()\n",
        ")\n",
        "\n",
        "invalid_records_df = sales_df_flagged.filter(F.col(\"is_invalid_amount\") | F.col(\"is_invalid_date\"))\n",
        "valid_records_df   = sales_df_flagged.filter(~(F.col(\"is_invalid_amount\") | F.col(\"is_invalid_date\")))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'F' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3840773517.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m parsed_timestamp = F.coalesce(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtry_to_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"txn_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yyyy-MM-dd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtry_to_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"txn_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dd/MM/yyyy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtry_to_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"txn_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yyyy/MM/dd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtry_to_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"txn_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dd-MM-yyyy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cleaned_df = sales_df_flagged.select(\n",
        "    F.trim(F.col(\"txn_id\")).alias(\"txn_id\"),\n",
        "    F.trim(F.col(\"city\")).alias(\"city\"),\n",
        "    F.trim(F.col(\"product\")).alias(\"product\"),\n",
        "    F.trim(F.col(\"category\")).alias(\"category\"),\n",
        "    \"amount\",\n",
        "    \"amount_int\",\n",
        "    \"txn_date\",\n",
        "    \"event_date\",\n",
        "    \"status\",\n",
        "    \"is_invalid_amount\",\n",
        "    \"is_invalid_date\"\n",
        ")"
      ],
      "metadata": {
        "id": "9wMxWoMFnZDc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert category to uppercase"
      ],
      "metadata": {
        "id": "gmsyyX0fplXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cleaned_df = cleaned_df.withColumn(\n",
        "    \"category\", F.upper(F.regexp_replace(F.col(\"category\"), r\"\\s+\", \"\"))  # remove inner spaces then uppercase\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZwoDYNvRpm1G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Handle invalid and null amounts"
      ],
      "metadata": {
        "id": "GIUogowRptTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Keep rows but mark; we'll filter nulls for analytics\n",
        "cleaned_df = cleaned_df.withColumn(\"amount_valid\", F.col(\"amount_int\").isNotNull())\n"
      ],
      "metadata": {
        "id": "2NK2JuhbpzoI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Parse multiple date formats into DateType"
      ],
      "metadata": {
        "id": "2MkDAHpzp3vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df = cleaned_df.withColumn(\"event_date\", F.col(\"event_date\").cast(DateType()))"
      ],
      "metadata": {
        "id": "GKMwLCLHp854"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Remove duplicate transactions"
      ],
      "metadata": {
        "id": "ybo242YSqBNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assume txn_id is unique; keep first occurrence\n",
        "dedup_df = cleaned_df.dropDuplicates([\"txn_id\"])\n"
      ],
      "metadata": {
        "id": "k8b8AFquqGiY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Keep only Completed transactions"
      ],
      "metadata": {
        "id": "nwoVISKWqLfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final_sales_df = dedup_df.filter(F.upper(F.col(\"status\")) == \"COMPLETED\")\n",
        "# Also ensure amount is valid and positive\n",
        "final_sales_df = final_sales_df.filter(F.col(\"amount_int\").isNotNull() & (F.col(\"amount_int\") > 0))\n"
      ],
      "metadata": {
        "id": "sY_M8JqqqOtu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 3 — DATA ENRICHMENT & JOINS\n",
        "12. Join sales data with city lookup"
      ],
      "metadata": {
        "id": "KBRKqwR4qSsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prepare lookup (trim city)\n",
        "city_df_clean = city_df.select(F.trim(F.col(\"city\")).alias(\"city\"), F.col(\"tier\"))\n",
        "sales_enriched_df = final_sales_df.join(city_df_clean, on=\"city\", how=\"left\")\n"
      ],
      "metadata": {
        "id": "H6u4Vx7sqW9d"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Use broadcast join where appropriate"
      ],
      "metadata": {
        "id": "8n-wHkSEqilW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import broadcast\n",
        "sales_enriched_df = final_sales_df.join(broadcast(city_df_clean), on=\"city\", how=\"left\")\n"
      ],
      "metadata": {
        "id": "BtzuuxOKqk8A"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Explain join strategy used"
      ],
      "metadata": {
        "id": "VA_fdQv8qo_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_enriched_df.explain(True)  # Expect BroadcastHashJoin since city_df is small and broadcasted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g05N2kfgqs4w",
        "outputId": "b35baec6-8d3f-4b75-f916-4f15e4cd257c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(LeftOuter, [city])\n",
            ":- Filter (isnotnull(amount_int#12) AND (amount_int#12 > 0))\n",
            ":  +- Filter (upper(status#6) = COMPLETED)\n",
            ":     +- Deduplicate [txn_id#16]\n",
            ":        +- Project [txn_id#16, city#17, product#18, category#20, amount#4, amount_int#12, txn_date#5, cast(event_date#13 as date) AS event_date#22, status#6, is_invalid_amount#14, is_invalid_date#15, amount_valid#21]\n",
            ":           +- Project [txn_id#16, city#17, product#18, category#20, amount#4, amount_int#12, txn_date#5, event_date#13, status#6, is_invalid_amount#14, is_invalid_date#15, isnotnull(amount_int#12) AS amount_valid#21]\n",
            ":              +- Project [txn_id#16, city#17, product#18, upper(regexp_replace(category#19, \\s+, , 1)) AS category#20, amount#4, amount_int#12, txn_date#5, event_date#13, status#6, is_invalid_amount#14, is_invalid_date#15]\n",
            ":                 +- Project [trim(txn_id#0, None) AS txn_id#16, trim(city#1, None) AS city#17, trim(product#2, None) AS product#18, trim(category#3, None) AS category#19, amount#4, amount_int#12, txn_date#5, event_date#13, status#6, is_invalid_amount#14, is_invalid_date#15]\n",
            ":                    +- Project [txn_id#0, city#1, product#2, category#3, amount#4, txn_date#5, status#6, amount_int#12, event_date#13, is_invalid_amount#14, isnull(event_date#13) AS is_invalid_date#15]\n",
            ":                       +- Project [txn_id#0, city#1, product#2, category#3, amount#4, txn_date#5, status#6, amount_int#12, event_date#13, isnull(amount_int#12) AS is_invalid_amount#14]\n",
            ":                          +- Project [txn_id#0, city#1, product#2, category#3, amount#4, txn_date#5, status#6, amount_int#12, coalesce(to_date(txn_date#5, Some(yyyy-MM-dd), Some(Etc/UTC), true), to_date(txn_date#5, Some(dd/MM/yyyy), Some(Etc/UTC), true), to_date(txn_date#5, Some(yyyy/MM/dd), Some(Etc/UTC), true), to_date(txn_date#5, Some(dd-MM-yyyy), Some(Etc/UTC), true)) AS event_date#13]\n",
            ":                             +- Project [txn_id#0, city#1, product#2, category#3, amount#4, txn_date#5, status#6, CASE WHEN isnull(amount#4) THEN cast(null as int) ELSE CASE WHEN (length(regexp_replace(amount#4, [^0-9], , 1)) > 0) THEN cast(regexp_replace(amount#4, [^0-9], , 1) as int) ELSE cast(null as int) END END AS amount_int#12]\n",
            ":                                +- LogicalRDD [txn_id#0, city#1, product#2, category#3, amount#4, txn_date#5, status#6], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- Project [trim(city#10, None) AS city#23, tier#11]\n",
            "      +- LogicalRDD [city#10, tier#11], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, txn_id: string, product: string, category: string, amount: string, amount_int: int, txn_date: string, event_date: date, status: string, is_invalid_amount: boolean, is_invalid_date: boolean, amount_valid: boolean, tier: string\n",
            "Project [city#17, txn_id#16, product#18, category#20, amount#4, amount_int#12, txn_date#5, event_date#22, status#6, is_invalid_amount#14, is_invalid_date#15, amount_valid#21, tier#11]\n",
            "+- Join LeftOuter, (city#17 = city#23)\n",
            "   :- Filter (isnotnull(amount_int#12) AND (amount_int#12 > 0))\n",
            "   :  +- Filter (upper(status#6) = COMPLETED)\n",
            "   :     +- Deduplicate [txn_id#16]\n",
            "   :        +- Project [txn_id#16, city#17, product#18, category#20, amount#4, amount_int#12, txn_date#5, cast(event_date#13 as date) AS event_date#22, status#6, is_invalid_amount#14, is_invalid_date#15, amount_valid#21]\n",
            "   :           +- Project [txn_id#16, city#17, product#18, category#20, amount#4, amount_int#12, txn_date#5, event_date#13, status#6, is_invalid_amount#14, is_invalid_date#15, isnotnull(amount_int#12) AS amount_valid#21]\n",
            "   :              +- Project [txn_id#16, city#17, product#18, upper(regexp_replace(category#19, \\s+, , 1)) AS category#20, amount#4, amount_int#12, txn_date#5, event_date#13, status#6, is_invalid_amount#14, is_invalid_date#15]\n",
            "   :                 +- Project [trim(txn_id#0, None) AS txn_id#16, trim(city#1, None) AS city#17, trim(product#2, None) AS product#18, trim(category#3, None) AS category#19, amount#4, amount_int#12, txn_date#5, event_date#13, status#6, is_invalid_amount#14, is_invalid_date#15]\n",
            "   :                    +- Project [txn_id#0, city#1, product#2, category#3, amount#4, txn_date#5, status#6, amount_int#12, event_date#13, is_invalid_amount#14, isnull(event_date#13) AS is_invalid_date#15]\n",
            "   :                       +- Project [txn_id#0, city#1, product#2, category#3, amount#4, txn_date#5, status#6, amount_int#12, event_date#13, isnull(amount_int#12) AS is_invalid_amount#14]\n",
            "   :                          +- Project [txn_id#0, city#1, product#2, category#3, amount#4, txn_date#5, status#6, amount_int#12, coalesce(to_date(txn_date#5, Some(yyyy-MM-dd), Some(Etc/UTC), true), to_date(txn_date#5, Some(dd/MM/yyyy), Some(Etc/UTC), true), to_date(txn_date#5, Some(yyyy/MM/dd), Some(Etc/UTC), true), to_date(txn_date#5, Some(dd-MM-yyyy), Some(Etc/UTC), true)) AS event_date#13]\n",
            "   :                             +- Project [txn_id#0, city#1, product#2, category#3, amount#4, txn_date#5, status#6, CASE WHEN isnull(amount#4) THEN cast(null as int) ELSE CASE WHEN (length(regexp_replace(amount#4, [^0-9], , 1)) > 0) THEN cast(regexp_replace(amount#4, [^0-9], , 1) as int) ELSE cast(null as int) END END AS amount_int#12]\n",
            "   :                                +- LogicalRDD [txn_id#0, city#1, product#2, category#3, amount#4, txn_date#5, status#6], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- Project [trim(city#10, None) AS city#23, tier#11]\n",
            "         +- LogicalRDD [city#10, tier#11], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#25, txn_id#16, product#27, category#29, amount#31, amount_int#33, txn_date#35, event_date#37, status#39, is_invalid_amount#41, is_invalid_date#43, amount_valid#45, tier#11]\n",
            "+- Join LeftOuter, (city#25 = city#23), rightHint=(strategy=broadcast)\n",
            "   :- Filter (isnotnull(status#39) AND ((upper(status#39) = COMPLETED) AND (isnotnull(amount_int#33) AND (amount_int#33 > 0))))\n",
            "   :  +- Aggregate [txn_id#16], [txn_id#16, first(city#17, false) AS city#25, first(product#18, false) AS product#27, first(category#20, false) AS category#29, first(amount#4, false) AS amount#31, first(amount_int#12, false) AS amount_int#33, first(txn_date#5, false) AS txn_date#35, first(event_date#13, false) AS event_date#37, first(status#6, false) AS status#39, first(is_invalid_amount#14, false) AS is_invalid_amount#41, first(is_invalid_date#15, false) AS is_invalid_date#43, first(amount_valid#21, false) AS amount_valid#45]\n",
            "   :     +- Project [trim(txn_id#0, None) AS txn_id#16, trim(city#1, None) AS city#17, trim(product#2, None) AS product#18, upper(regexp_replace(trim(category#3, None), \\s+, , 1)) AS category#20, amount#4, amount_int#12, txn_date#5, event_date#13, status#6, isnull(amount_int#12) AS is_invalid_amount#14, isnull(event_date#13) AS is_invalid_date#15, isnotnull(amount_int#12) AS amount_valid#21]\n",
            "   :        +- Project [txn_id#0, city#1, product#2, category#3, amount#4, txn_date#5, status#6, CASE WHEN isnull(amount#4) THEN null ELSE CASE WHEN (length(regexp_replace(amount#4, [^0-9], , 1)) > 0) THEN cast(regexp_replace(amount#4, [^0-9], , 1) as int) END END AS amount_int#12, coalesce(cast(gettimestamp(txn_date#5, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date), cast(gettimestamp(txn_date#5, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date), cast(gettimestamp(txn_date#5, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date), cast(gettimestamp(txn_date#5, dd-MM-yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date)) AS event_date#13]\n",
            "   :           +- LogicalRDD [txn_id#0, city#1, product#2, category#3, amount#4, txn_date#5, status#6], false\n",
            "   +- Project [trim(city#10, None) AS city#23, tier#11]\n",
            "      +- Filter isnotnull(trim(city#10, None))\n",
            "         +- LogicalRDD [city#10, tier#11], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#25, txn_id#16, product#27, category#29, amount#31, amount_int#33, txn_date#35, event_date#37, status#39, is_invalid_amount#41, is_invalid_date#43, amount_valid#45, tier#11]\n",
            "   +- BroadcastHashJoin [city#25], [city#23], LeftOuter, BuildRight, false\n",
            "      :- Filter (isnotnull(status#39) AND ((upper(status#39) = COMPLETED) AND (isnotnull(amount_int#33) AND (amount_int#33 > 0))))\n",
            "      :  +- SortAggregate(key=[txn_id#16], functions=[first(city#17, false), first(product#18, false), first(category#20, false), first(amount#4, false), first(amount_int#12, false), first(txn_date#5, false), first(event_date#13, false), first(status#6, false), first(is_invalid_amount#14, false), first(is_invalid_date#15, false), first(amount_valid#21, false)], output=[txn_id#16, city#25, product#27, category#29, amount#31, amount_int#33, txn_date#35, event_date#37, status#39, is_invalid_amount#41, is_invalid_date#43, amount_valid#45])\n",
            "      :     +- Sort [txn_id#16 ASC NULLS FIRST], false, 0\n",
            "      :        +- Exchange hashpartitioning(txn_id#16, 200), ENSURE_REQUIREMENTS, [plan_id=46]\n",
            "      :           +- SortAggregate(key=[txn_id#16], functions=[partial_first(city#17, false), partial_first(product#18, false), partial_first(category#20, false), partial_first(amount#4, false), partial_first(amount_int#12, false), partial_first(txn_date#5, false), partial_first(event_date#13, false), partial_first(status#6, false), partial_first(is_invalid_amount#14, false), partial_first(is_invalid_date#15, false), partial_first(amount_valid#21, false)], output=[txn_id#16, first#68, valueSet#69, first#70, valueSet#71, first#72, valueSet#73, first#74, valueSet#75, first#76, valueSet#77, first#78, valueSet#79, first#80, valueSet#81, first#82, valueSet#83, first#84, valueSet#85, first#86, valueSet#87, first#88, valueSet#89])\n",
            "      :              +- Sort [txn_id#16 ASC NULLS FIRST], false, 0\n",
            "      :                 +- Project [trim(txn_id#0, None) AS txn_id#16, trim(city#1, None) AS city#17, trim(product#2, None) AS product#18, upper(regexp_replace(trim(category#3, None), \\s+, , 1)) AS category#20, amount#4, amount_int#12, txn_date#5, event_date#13, status#6, isnull(amount_int#12) AS is_invalid_amount#14, isnull(event_date#13) AS is_invalid_date#15, isnotnull(amount_int#12) AS amount_valid#21]\n",
            "      :                    +- Project [txn_id#0, city#1, product#2, category#3, amount#4, txn_date#5, status#6, CASE WHEN isnull(amount#4) THEN null ELSE CASE WHEN (length(regexp_replace(amount#4, [^0-9], , 1)) > 0) THEN cast(regexp_replace(amount#4, [^0-9], , 1) as int) END END AS amount_int#12, coalesce(cast(gettimestamp(txn_date#5, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date), cast(gettimestamp(txn_date#5, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date), cast(gettimestamp(txn_date#5, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date), cast(gettimestamp(txn_date#5, dd-MM-yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date)) AS event_date#13]\n",
            "      :                       +- Scan ExistingRDD[txn_id#0,city#1,product#2,category#3,amount#4,txn_date#5,status#6]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=51]\n",
            "         +- Project [trim(city#10, None) AS city#23, tier#11]\n",
            "            +- Filter isnotnull(trim(city#10, None))\n",
            "               +- Scan ExistingRDD[city#10,tier#11]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 4 — ANALYTICS & WINDOW FUNCTIONS\n",
        "16. Revenue per city"
      ],
      "metadata": {
        "id": "2lfpKMmdqw8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "revenue_by_city_df = sales_enriched_df.groupBy(\"city\").agg(\n",
        "    F.sum(\"amount_int\").alias(\"total_revenue\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "Li_DfD0_wo2g"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Revenue per product"
      ],
      "metadata": {
        "id": "_pRKy7k8wuk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "revenue_by_product_df = sales_enriched_df.groupBy(\"product\").agg(\n",
        "    F.sum(\"amount_int\").alias(\"total_revenue\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "f_xgmIkcwvnv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Rank cities by total revenue"
      ],
      "metadata": {
        "id": "vv7eIU7ow0DA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "w_city_rank = Window.orderBy(F.desc(\"total_revenue\"))\n",
        "city_rank_df = revenue_by_city_df.withColumn(\"city_rank\", F.dense_rank().over(w_city_rank))\n",
        "\n"
      ],
      "metadata": {
        "id": "pp6QpajCw7CZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Rank products within each city"
      ],
      "metadata": {
        "id": "PbjyFUfbxHcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "revenue_city_product_df = sales_enriched_df.groupBy(\"city\", \"product\").agg(\n",
        "    F.sum(\"amount_int\").alias(\"product_revenue\")\n",
        ")\n",
        "w_product_in_city = Window.partitionBy(\"city\").orderBy(F.desc(\"product_revenue\"))\n",
        "product_rank_in_city_df = revenue_city_product_df.withColumn(\n",
        "    \"product_rank_in_city\", F.dense_rank().over(w_product_in_city)\n",
        ")\n"
      ],
      "metadata": {
        "id": "KbggXXpIxIpL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Identify top-performing city per day\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8Fa3UbYxNOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "daily_city_revenue_df = sales_enriched_df.groupBy(\"event_date\", \"city\").agg(\n",
        "    F.sum(\"amount_int\").alias(\"daily_revenue\")\n",
        ")\n",
        "w_top_city_per_day = Window.partitionBy(\"event_date\").orderBy(F.desc(\"daily_revenue\"))\n",
        "top_city_per_day_df = daily_city_revenue_df.withColumn(\n",
        "    \"rn\", F.row_number().over(w_top_city_per_day)\n",
        ").filter(F.col(\"rn\") == 1).drop(\"rn\")\n",
        "\n"
      ],
      "metadata": {
        "id": "DELkrTThxWcF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Identify reusable DataFrames"
      ],
      "metadata": {
        "id": "GmK9R0IBxw9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Apply caching appropriately\n",
        "\n",
        "\n",
        "final_sales_df.cache()\n",
        "sales_enriched_df.cache()\n",
        "# Materialize cache\n",
        "final_sales_df.count(); sales_enriched_df.count()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "UCiGvlefxyEj",
        "outputId": "cc2b9e8c-289c-4ba2-98e8-e8a9a061d4a1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DateTimeException",
          "evalue": "[CANNOT_PARSE_TIMESTAMP] Text '05/01/2024' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDateTimeException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-279478468.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msales_enriched_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Materialize cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfinal_sales_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0msales_enriched_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDateTimeException\u001b[0m: [CANNOT_PARSE_TIMESTAMP] Text '05/01/2024' could not be parsed at index 0. Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007"
          ]
        }
      ]
    }
  ]
}