{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vem4wuWhzC-9"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"Orders\") \\\n",
        ".getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orders_data = [\n",
        "(\"O001\",\"Delhi \",\"Laptop\",\"45000\",\"2024-01-05\",\"Completed\"),\n",
        "(\"O002\",\"Mumbai\",\"Mobile \",\"32000\",\"05/01/2024\",\"Completed\"),\n",
        "(\"O003\",\"Bangalore\",\"Tablet\",\"30000\",\"2024/01/06\",\"Completed\"),\n",
        "(\"O004\",\"Delhi\",\"Laptop\",\"\",\"2024-01-07\",\"Cancelled\"),\n",
        "(\"O005\",\"Mumbai\",\"Mobile\",\"invalid\",\"2024-01-08\",\"Completed\"),\n",
        "(\"O006\",\"Chennai\",\"Tablet\",None,\"2024-01-08\",\"Completed\"),\n",
        "(\"O007\",\"Delhi\",\"Laptop\",\"47000\",\"09-01-2024\",\"Completed\"),\n",
        "(\"O008\",\"Bangalore\",\"Mobile\",\"28000\",\"2024-01-09\",\"Completed\"),\n",
        "(\"O009\",\"Mumbai\",\"Laptop\",\"55000\",\"2024-01-10\",\"Completed\"),\n",
        "(\"O009\",\"Mumbai\",\"Laptop\",\"55000\",\"2024-01-10\",\"Completed\")\n",
        "]\n",
        "\n",
        "columns = [\"order_id\" ,\"city\" ,  \"product\" ,\"amount\" ,\"order_date\" ,\"status\" ]\n",
        "df = spark.createDataFrame(orders_data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggtXD8BC4NDk",
        "outputId": "834f170d-d32f-499b-be19-73989d9de92a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+-------+-------+----------+---------+\n",
            "|order_id|     city|product| amount|order_date|   status|\n",
            "+--------+---------+-------+-------+----------+---------+\n",
            "|    O001|   Delhi | Laptop|  45000|2024-01-05|Completed|\n",
            "|    O002|   Mumbai|Mobile |  32000|05/01/2024|Completed|\n",
            "|    O003|Bangalore| Tablet|  30000|2024/01/06|Completed|\n",
            "|    O004|    Delhi| Laptop|       |2024-01-07|Cancelled|\n",
            "|    O005|   Mumbai| Mobile|invalid|2024-01-08|Completed|\n",
            "|    O006|  Chennai| Tablet|   NULL|2024-01-08|Completed|\n",
            "|    O007|    Delhi| Laptop|  47000|09-01-2024|Completed|\n",
            "|    O008|Bangalore| Mobile|  28000|2024-01-09|Completed|\n",
            "|    O009|   Mumbai| Laptop|  55000|2024-01-10|Completed|\n",
            "|    O009|   Mumbai| Laptop|  55000|2024-01-10|Completed|\n",
            "+--------+---------+-------+-------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 1 — DATA INGESTION & SCHEMA"
      ],
      "metadata": {
        "id": "DEm309R55k0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1: Define Explicit Schema"
      ],
      "metadata": {
        "id": "lLz-klkK5nqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"order_id\", StringType(), True),\n",
        "    StructField(\"city\", StringType(), True),\n",
        "    StructField(\"product\", StringType(), True),\n",
        "    StructField(\"amount\", StringType(), True),\n",
        "    StructField(\"order_date\", StringType(), True),\n",
        "    StructField(\"status\", StringType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "UqtatDBq57nh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2: Create DataFrame Using Schema"
      ],
      "metadata": {
        "id": "2eF6KkXT6Lzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data=orders_data, schema=schema)\n"
      ],
      "metadata": {
        "id": "hn9PDQ7S6OvO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Print schema and validate data types"
      ],
      "metadata": {
        "id": "DHAvi5zn6lLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T43xxjnD6pfP",
        "outputId": "9256b1ed-4a4a-42e4-ed29-571ca68d0e37"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIWVR8Wg6ygA",
        "outputId": "58e213c3-6b43-4f29-d61c-351bb5f8e9ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+-------+-------+----------+---------+\n",
            "|order_id|city     |product|amount |order_date|status   |\n",
            "+--------+---------+-------+-------+----------+---------+\n",
            "|O001    |Delhi    |Laptop |45000  |2024-01-05|Completed|\n",
            "|O002    |Mumbai   |Mobile |32000  |05/01/2024|Completed|\n",
            "|O003    |Bangalore|Tablet |30000  |2024/01/06|Completed|\n",
            "|O004    |Delhi    |Laptop |       |2024-01-07|Cancelled|\n",
            "|O005    |Mumbai   |Mobile |invalid|2024-01-08|Completed|\n",
            "|O006    |Chennai  |Tablet |NULL   |2024-01-08|Completed|\n",
            "|O007    |Delhi    |Laptop |47000  |09-01-2024|Completed|\n",
            "|O008    |Bangalore|Mobile |28000  |2024-01-09|Completed|\n",
            "|O009    |Mumbai   |Laptop |55000  |2024-01-10|Completed|\n",
            "|O009    |Mumbai   |Laptop |55000  |2024-01-10|Completed|\n",
            "+--------+---------+-------+-------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 2 — DATA CLEANING"
      ],
      "metadata": {
        "id": "-5vu4fZH61Tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Trim all string columns"
      ],
      "metadata": {
        "id": "msFSLA8M7WQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, trim\n",
        "\n",
        "df = df.withColumn(\"order_id\", trim(col(\"order_id\"))) \\\n",
        "       .withColumn(\"city\", trim(col(\"city\"))) \\\n",
        "       .withColumn(\"product\", trim(col(\"product\"))) \\\n",
        "       .withColumn(\"amount\", trim(col(\"amount\"))) \\\n",
        "       .withColumn(\"order_date\", trim(col(\"order_date\"))) \\\n",
        "       .withColumn(\"status\", trim(col(\"status\")))\n"
      ],
      "metadata": {
        "id": "vWMHM3ll7ad-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Standardize city and product values"
      ],
      "metadata": {
        "id": "eC7aWt_K7egN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import initcap\n",
        "\n",
        "df = df.withColumn(\"city\", initcap(col(\"city\"))) \\\n",
        "       .withColumn(\"product\", initcap(col(\"product\")))\n"
      ],
      "metadata": {
        "id": "EBGLMPvS7j7t"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Convert amount to IntegerType"
      ],
      "metadata": {
        "id": "2leDGgaB71vI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import when\n"
      ],
      "metadata": {
        "id": "ReI5GsgE73Od"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\n",
        "    \"amount_int\",\n",
        "    col(\"amount\").cast(IntegerType())\n",
        ")"
      ],
      "metadata": {
        "id": "VoNUmw3D8GpG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Handle invalid and null amount values"
      ],
      "metadata": {
        "id": "PQc1BxSn8JMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\n",
        "    \"amount_int\",\n",
        "    when(col(\"amount_int\").isNull(), 0)\n",
        "    .otherwise(col(\"amount_int\"))\n",
        ")\n"
      ],
      "metadata": {
        "id": "zx5Mt8T_8PQg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Remove duplicate orders"
      ],
      "metadata": {
        "id": "hFCfuS-g8Tx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropDuplicates([\"order_id\"])\n"
      ],
      "metadata": {
        "id": "636_Cxlm8Yzz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Keep only Completed orders"
      ],
      "metadata": {
        "id": "CVqTnjpD8b4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.filter(col(\"status\") == \"Completed\")\n"
      ],
      "metadata": {
        "id": "une6pIZI8fop"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 3 — BASIC ANALYTICS"
      ],
      "metadata": {
        "id": "X7b9CJzL8jgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, col, when, lit, coalesce\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Re-define the column used for aggregation to correctly handle malformed strings.\n",
        "# This ensures that empty strings and 'invalid' are treated as NULL before casting.\n",
        "corrected_amount_for_sum = when(col(\"amount\") == \"\", lit(None)) \\\n",
        "                           .when(col(\"amount\") == \"invalid\", lit(None)) \\\n",
        "                           .otherwise(col(\"amount\"))\n",
        "\n",
        "# Now, cast to IntegerType, and then replace any resulting NULLs with 0.\n",
        "# The initial replacement of '' and 'invalid' with None helps the cast function.\n",
        "corrected_amount_for_sum = coalesce(corrected_amount_for_sum.cast(IntegerType()), lit(0))\n",
        "\n",
        "revenue_per_city = df.groupBy(\"city\") \\\n",
        "    .agg(sum(corrected_amount_for_sum).alias(\"total_revenue\"))\n",
        "\n",
        "revenue_per_city.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1XffsBn9BqS",
        "outputId": "9a572125-07cb-47e7-efac-06f00190f2cc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+\n",
            "|     city|total_revenue|\n",
            "+---------+-------------+\n",
            "|Bangalore|        58000|\n",
            "|  Chennai|            0|\n",
            "|   Mumbai|        87000|\n",
            "|    Delhi|        92000|\n",
            "+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, col, when, lit, coalesce\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Re-define the column used for aggregation to correctly handle malformed strings\n",
        "# by explicitly converting '' and 'invalid' to None before casting.\n",
        "corrected_amount_for_sum = when(col(\"amount\") == \"\", lit(None)) \\\n",
        "                           .when(col(\"amount\") == \"invalid\", lit(None)) \\\n",
        "                           .otherwise(col(\"amount\"))\n",
        "\n",
        "# Now, cast to IntegerType, and then replace any resulting NULLs with 0 for aggregation.\n",
        "corrected_amount_for_sum = coalesce(corrected_amount_for_sum.cast(IntegerType()), lit(0))\n",
        "\n",
        "revenue_per_product = df.groupBy(\"product\") \\\n",
        "    .agg(sum(corrected_amount_for_sum).alias(\"total_revenue\"))\n",
        "\n",
        "revenue_per_product.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8ou641B9mBb",
        "outputId": "135afc84-2035-46f8-b0c4-90b9e170d01d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+\n",
            "|product|total_revenue|\n",
            "+-------+-------------+\n",
            "| Laptop|       147000|\n",
            "| Mobile|        60000|\n",
            "| Tablet|        30000|\n",
            "+-------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg, col, when, lit, coalesce\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Create a robust numeric column that handles malformed string inputs\n",
        "corrected_amount_for_avg = when(col(\"amount\") == \"\", lit(None)) \\\n",
        "                           .when(col(\"amount\") == \"invalid\", lit(None)) \\\n",
        "                           .otherwise(col(\"amount\"))\n",
        "\n",
        "# Cast to IntegerType, and then replace any resulting NULLs with 0 for averaging.\n",
        "# Using 0 is appropriate for average when NULLs should not contribute or indicate a zero value.\n",
        "corrected_amount_for_avg = coalesce(corrected_amount_for_avg.cast(IntegerType()), lit(0))\n",
        "\n",
        "avg_order_value_city = df.groupBy(\"city\") \\\n",
        "    .agg(avg(corrected_amount_for_avg).alias(\"avg_order_value\"))\n",
        "\n",
        "avg_order_value_city.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mpTwrlq9zL3",
        "outputId": "8e733fe8-6ad0-43fc-d3b1-15021893fe73"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------------+\n",
            "|     city|avg_order_value|\n",
            "+---------+---------------+\n",
            "|Bangalore|        29000.0|\n",
            "|  Chennai|            0.0|\n",
            "|   Mumbai|        29000.0|\n",
            "|    Delhi|        46000.0|\n",
            "+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 4 — WINDOW FUNCTION\n",
        "\n"
      ],
      "metadata": {
        "id": "PSfcNLrA-Xxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, rank, col, when, lit, coalesce\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Create a robust numeric column that handles malformed string inputs\n",
        "corrected_amount_for_sum = when(col(\"amount\") == \"\", lit(None)) \\\n",
        "                           .when(col(\"amount\") == \"invalid\", lit(None)) \\\n",
        "                           .otherwise(col(\"amount\"))\n",
        "\n",
        "# Cast to IntegerType, and then replace any resulting NULLs with 0 for aggregation.\n",
        "corrected_amount_for_sum = coalesce(corrected_amount_for_sum.cast(IntegerType()), lit(0))\n",
        "\n",
        "city_revenue_df = df.groupBy(\"city\") \\\n",
        "    .agg(sum(corrected_amount_for_sum).alias(\"total_revenue\"))\n",
        "\n",
        "window_spec = Window.orderBy(col(\"total_revenue\").desc())\n",
        "\n",
        "ranked_cities = city_revenue_df.withColumn(\n",
        "    \"city_rank\",\n",
        "    rank().over(window_spec)\n",
        ")\n",
        "\n",
        "ranked_cities.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkyUJWfX-ayf",
        "outputId": "6a0db621-4f51-4885-bb71-e6d6bf0eb1b0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+---------+\n",
            "|     city|total_revenue|city_rank|\n",
            "+---------+-------------+---------+\n",
            "|    Delhi|        92000|        1|\n",
            "|   Mumbai|        87000|        2|\n",
            "|Bangalore|        58000|        3|\n",
            "|  Chennai|            0|        4|\n",
            "+---------+-------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_city = ranked_cities.filter(col(\"city_rank\") == 1)\n",
        "\n",
        "top_city.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGTEHene-lHd",
        "outputId": "65eaea6d-e610-44c1-93f8-35fcc07d13f8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------+---------+\n",
            "| city|total_revenue|city_rank|\n",
            "+-----+-------------+---------+\n",
            "|Delhi|        92000|        1|\n",
            "+-----+-------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PHASE 5 — PERFORMANCE AWARENESS"
      ],
      "metadata": {
        "id": "d1UVSkmL-s57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mG1qEoIX-xeQ",
        "outputId": "31e6edce-5e45-424e-92c3-afbc09cf68f1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[order_id: string, city: string, product: string, amount: string, order_date: string, status: string, amount_int: int]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, col, when, lit, coalesce, avg\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "df.unpersist() # Unpersist to clear any old cached state that might be causing issues.\n",
        "\n",
        "# Create a robust numeric column that handles malformed string inputs\n",
        "# and can be used for both sum and average calculations.\n",
        "corrected_amount_for_agg = when(col(\"amount\") == \"\", lit(None)) \\\n",
        "                           .when(col(\"amount\") == \"invalid\", lit(None)) \\\n",
        "                           .otherwise(col(\"amount\"))\n",
        "\n",
        "# Cast to IntegerType, and then replace any resulting NULLs with 0 for aggregation.\n",
        "corrected_amount_for_agg = coalesce(corrected_amount_for_agg.cast(IntegerType()), lit(0))\n",
        "\n",
        "# Aggregation 1\n",
        "df.groupBy(\"city\") \\\n",
        "  .agg(sum(corrected_amount_for_agg).alias(\"city_revenue\")) \\\n",
        "  .show()\n",
        "\n",
        "# Aggregation 2\n",
        "df.groupBy(\"product\") \\\n",
        "  .agg(sum(corrected_amount_for_agg).alias(\"product_revenue\")) \\\n",
        "  .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNBrZne--2Ou",
        "outputId": "bd4a26bf-2079-4530-b245-9e71b0357ea3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+\n",
            "|     city|city_revenue|\n",
            "+---------+------------+\n",
            "|Bangalore|       58000|\n",
            "|  Chennai|           0|\n",
            "|   Mumbai|       87000|\n",
            "|    Delhi|       92000|\n",
            "+---------+------------+\n",
            "\n",
            "+-------+---------------+\n",
            "|product|product_revenue|\n",
            "+-------+---------------+\n",
            "| Laptop|         147000|\n",
            "| Mobile|          60000|\n",
            "| Tablet|          30000|\n",
            "+-------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmtqPYza--9l",
        "outputId": "71f0c4c2-eb2c-47fa-a872-2a1d4e690a09"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Filter '`=`('status, Completed)\n",
            "+- Deduplicate [order_id#50]\n",
            "   +- Project [order_id#50, city#56, product#57, amount#53, order_date#54, status#55, CASE WHEN isnull(amount_int#58) THEN 0 ELSE amount_int#58 END AS amount_int#59]\n",
            "      +- Project [order_id#50, city#56, product#57, amount#53, order_date#54, status#55, cast(amount#53 as int) AS amount_int#58]\n",
            "         +- Project [order_id#50, city#56, initcap(product#52) AS product#57, amount#53, order_date#54, status#55]\n",
            "            +- Project [order_id#50, initcap(city#51) AS city#56, product#52, amount#53, order_date#54, status#55]\n",
            "               +- Project [order_id#50, city#51, product#52, amount#53, order_date#54, trim(status#30, None) AS status#55]\n",
            "                  +- Project [order_id#50, city#51, product#52, amount#53, trim(order_date#29, None) AS order_date#54, status#30]\n",
            "                     +- Project [order_id#50, city#51, product#52, trim(amount#28, None) AS amount#53, order_date#29, status#30]\n",
            "                        +- Project [order_id#50, city#51, trim(product#27, None) AS product#52, amount#28, order_date#29, status#30]\n",
            "                           +- Project [order_id#50, trim(city#26, None) AS city#51, product#27, amount#28, order_date#29, status#30]\n",
            "                              +- Project [trim(order_id#25, None) AS order_id#50, city#26, product#27, amount#28, order_date#29, status#30]\n",
            "                                 +- LogicalRDD [order_id#25, city#26, product#27, amount#28, order_date#29, status#30], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "order_id: string, city: string, product: string, amount: string, order_date: string, status: string, amount_int: int\n",
            "Filter (status#55 = Completed)\n",
            "+- Deduplicate [order_id#50]\n",
            "   +- Project [order_id#50, city#56, product#57, amount#53, order_date#54, status#55, CASE WHEN isnull(amount_int#58) THEN 0 ELSE amount_int#58 END AS amount_int#59]\n",
            "      +- Project [order_id#50, city#56, product#57, amount#53, order_date#54, status#55, cast(amount#53 as int) AS amount_int#58]\n",
            "         +- Project [order_id#50, city#56, initcap(product#52) AS product#57, amount#53, order_date#54, status#55]\n",
            "            +- Project [order_id#50, initcap(city#51) AS city#56, product#52, amount#53, order_date#54, status#55]\n",
            "               +- Project [order_id#50, city#51, product#52, amount#53, order_date#54, trim(status#30, None) AS status#55]\n",
            "                  +- Project [order_id#50, city#51, product#52, amount#53, trim(order_date#29, None) AS order_date#54, status#30]\n",
            "                     +- Project [order_id#50, city#51, product#52, trim(amount#28, None) AS amount#53, order_date#29, status#30]\n",
            "                        +- Project [order_id#50, city#51, trim(product#27, None) AS product#52, amount#28, order_date#29, status#30]\n",
            "                           +- Project [order_id#50, trim(city#26, None) AS city#51, product#27, amount#28, order_date#29, status#30]\n",
            "                              +- Project [trim(order_id#25, None) AS order_id#50, city#26, product#27, amount#28, order_date#29, status#30]\n",
            "                                 +- LogicalRDD [order_id#25, city#26, product#27, amount#28, order_date#29, status#30], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "InMemoryRelation [order_id#50, city#56, product#57, amount#53, order_date#54, status#55, amount_int#59], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "   +- AdaptiveSparkPlan isFinalPlan=false\n",
            "      +- Filter (isnotnull(status#1315) AND (status#1315 = Completed))\n",
            "         +- SortAggregate(key=[order_id#50], functions=[first(city#56, false), first(product#57, false), first(amount#53, false), first(order_date#54, false), first(status#55, false), first(amount_int#59, false)], output=[order_id#50, city#1307, product#1309, amount#1311, order_date#1313, status#1315, amount_int#1317])\n",
            "            +- Sort [order_id#50 ASC NULLS FIRST], false, 0\n",
            "               +- Exchange hashpartitioning(order_id#50, 200), ENSURE_REQUIREMENTS, [plan_id=1651]\n",
            "                  +- SortAggregate(key=[order_id#50], functions=[partial_first(city#56, false), partial_first(product#57, false), partial_first(amount#53, false), partial_first(order_date#54, false), partial_first(status#55, false), partial_first(amount_int#59, false)], output=[order_id#50, first#1330, valueSet#1331, first#1332, valueSet#1333, first#1334, valueSet#1335, first#1336, valueSet#1337, first#1338, valueSet#1339, first#1340, valueSet#1341])\n",
            "                     +- Sort [order_id#50 ASC NULLS FIRST], false, 0\n",
            "                        +- Project [order_id#50, city#56, product#57, amount#53, order_date#54, status#55, CASE WHEN isnull(amount_int#58) THEN 0 ELSE amount_int#58 END AS amount_int#59]\n",
            "                           +- Project [order_id#50, city#56, product#57, amount#53, order_date#54, status#55, cast(amount#53 as int) AS amount_int#58]\n",
            "                              +- Project [trim(order_id#25, None) AS order_id#50, initcap(trim(city#26, None)) AS city#56, initcap(trim(product#27, None)) AS product#57, trim(amount#28, None) AS amount#53, trim(order_date#29, None) AS order_date#54, trim(status#30, None) AS status#55]\n",
            "                                 +- Scan ExistingRDD[order_id#25,city#26,product#27,amount#28,order_date#29,status#30]\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- InMemoryTableScan [order_id#50, city#56, product#57, amount#53, order_date#54, status#55, amount_int#59]\n",
            "      +- InMemoryRelation [order_id#50, city#56, product#57, amount#53, order_date#54, status#55, amount_int#59], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "            +- AdaptiveSparkPlan isFinalPlan=false\n",
            "               +- Filter (isnotnull(status#1315) AND (status#1315 = Completed))\n",
            "                  +- SortAggregate(key=[order_id#50], functions=[first(city#56, false), first(product#57, false), first(amount#53, false), first(order_date#54, false), first(status#55, false), first(amount_int#59, false)], output=[order_id#50, city#1307, product#1309, amount#1311, order_date#1313, status#1315, amount_int#1317])\n",
            "                     +- Sort [order_id#50 ASC NULLS FIRST], false, 0\n",
            "                        +- Exchange hashpartitioning(order_id#50, 200), ENSURE_REQUIREMENTS, [plan_id=1651]\n",
            "                           +- SortAggregate(key=[order_id#50], functions=[partial_first(city#56, false), partial_first(product#57, false), partial_first(amount#53, false), partial_first(order_date#54, false), partial_first(status#55, false), partial_first(amount_int#59, false)], output=[order_id#50, first#1330, valueSet#1331, first#1332, valueSet#1333, first#1334, valueSet#1335, first#1336, valueSet#1337, first#1338, valueSet#1339, first#1340, valueSet#1341])\n",
            "                              +- Sort [order_id#50 ASC NULLS FIRST], false, 0\n",
            "                                 +- Project [order_id#50, city#56, product#57, amount#53, order_date#54, status#55, CASE WHEN isnull(amount_int#58) THEN 0 ELSE amount_int#58 END AS amount_int#59]\n",
            "                                    +- Project [order_id#50, city#56, product#57, amount#53, order_date#54, status#55, cast(amount#53 as int) AS amount_int#58]\n",
            "                                       +- Project [trim(order_id#25, None) AS order_id#50, initcap(trim(city#26, None)) AS city#56, initcap(trim(product#27, None)) AS product#57, trim(amount#28, None) AS amount#53, trim(order_date#29, None) AS order_date#54, trim(status#30, None) AS status#55]\n",
            "                                          +- Scan ExistingRDD[order_id#25,city#26,product#27,amount#28,order_date#29,status#30]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}